{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-29T02:32:17.748766Z",
     "start_time": "2024-09-29T02:32:17.743549Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from flash_attn import flash_attn_with_kvcache\n",
    "\n",
    "from tokenizer import ChatFormat, Tokenizer\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflash_attn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flash_attn_with_kvcache\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtokenizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatFormat, Tokenizer\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tokenizer'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Housekeeping to load pretrained llama.\n",
    "device = 'cuda'\n",
    "model_name = 'Meta-Llama-3-8B-Instruct'\n",
    "tokenizer_path = f'{model_name}/original/tokenizer.model'\n",
    "tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "\n",
    "model = torch.load(f'{model_name}/original/consolidated.00.pth', map_location=device, mmap=False)\n",
    "\n",
    "with open(f'{model_name}/original/params.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "dim = config['dim']\n",
    "n_layers = config['n_layers']\n",
    "n_heads = config['n_heads']\n",
    "n_kv_heads = config['n_kv_heads']\n",
    "vocab_size = config['vocab_size']\n",
    "multiple_of = config['multiple_of']\n",
    "ffn_dim_multiplier = config['ffn_dim_multiplier']\n",
    "norm_eps = config['norm_eps']\n",
    "rope_theta = torch.tensor(config['rope_theta'], device=device)\n",
    "head_dim = dim // n_heads # 4096 // 32 = 128\n",
    "max_seq_len = 8192\n",
    "\n",
    "stop_tokens = torch.tensor(list(tokenizer.stop_tokens), device=device)\n",
    "\n",
    "# Set Embedding\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dim, device=device, _weight=model['tok_embeddings.weight'])\n",
    "\n",
    "# Precompute freqs cis for rope\n",
    "zero_to_one_split_into_64_parts = torch.tensor(range(head_dim//2), device=device)/(head_dim//2)\n",
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "freqs_for_each_token = torch.outer(torch.arange(max_seq_len, device=device), freqs)\n",
    "freqs_cis_max = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "\n",
    "# Utility funcs for rope\n",
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "    shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights\n",
    "\n",
    "# Generate next token i.e. do one forward pass of llama\n",
    "def forward(tokens, start_pos):\n",
    "    bsz, T = tokens.shape\n",
    "    final_embedding = embedding_layer(tokens)\n",
    "    freqs_cis = freqs_cis_max[start_pos:start_pos+T, :]\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        q_layer = model[f'layers.{layer}.attention.wq.weight']\n",
    "        k_layer = model[f'layers.{layer}.attention.wk.weight']\n",
    "        v_layer = model[f'layers.{layer}.attention.wv.weight']\n",
    "        w_layer = model[f'layers.{layer}.attention.wo.weight']\n",
    "\n",
    "        layer_embedding_norm = rms_norm(final_embedding, model[f'layers.{layer}.attention_norm.weight'])\n",
    "\n",
    "        q = layer_embedding_norm @ q_layer.T\n",
    "        k = layer_embedding_norm @ k_layer.T\n",
    "        v = layer_embedding_norm @ v_layer.T\n",
    "\n",
    "        q = q.view(bsz, T, n_heads, head_dim)\n",
    "        k = k.view(bsz, T, n_kv_heads, head_dim)\n",
    "        v = v.view(bsz, T, n_kv_heads, head_dim)\n",
    "\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cis)\n",
    "        \n",
    "        # Use flash attention with kv-cache support.\n",
    "        k_cache, v_cache = kv_cache[layer]\n",
    "        y = flash_attn_with_kvcache(q, k_cache, v_cache, k, v, cache_seqlens=start_pos, causal=True)\n",
    "\n",
    "        stacked_qkv_attention = y.view(bsz, T, dim)\n",
    "\n",
    "        embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
    "        embedding_after_edit = final_embedding + embedding_delta\n",
    "        embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f'layers.{layer}.ffn_norm.weight'])\n",
    "        w1 = model[f'layers.{layer}.feed_forward.w1.weight']\n",
    "        w2 = model[f'layers.{layer}.feed_forward.w2.weight']\n",
    "        w3 = model[f'layers.{layer}.feed_forward.w3.weight']\n",
    "        output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "        final_embedding = embedding_after_edit + output_after_feedforward\n",
    "\n",
    "    final_embedding = rms_norm(final_embedding, model['norm.weight'])\n",
    "    logits = torch.matmul(final_embedding[:,-1,:], model['output.weight'].T)\n",
    "    tokens = torch.argmax(logits, dim=-1)\n",
    "    return tokens\n",
    "\n",
    "# Load ShareGPT prompts\n",
    "with open('sharegpt-filtered.json') as f:\n",
    "    sharegpt = json.load(f)\n",
    "\n",
    "requests = []\n",
    "for i in range(len(sharegpt)):\n",
    "    conversations = sharegpt[i]['conversations']\n",
    "    if len(conversations) > 0:\n",
    "        requests.append([{'role': 'user', 'content': sharegpt[i]['conversations'][0]['value']}])\n",
    "\n",
    "# Use given amount of requests\n",
    "num_requests = int(sys.argv[1])\n",
    "dialogs = requests[:num_requests]\n",
    "\n",
    "# Tokenize\n",
    "prompt_tokens = [ChatFormat(tokenizer).encode_dialog_prompt(d) for d in dialogs]\n",
    "bsz = len(prompt_tokens)\n",
    "min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "\n",
    "tokens = torch.full((bsz, max_seq_len), tokenizer.pad_id, dtype=torch.long, device=device)\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, :len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "\n",
    "prev_pos = 0\n",
    "eos_reached = torch.tensor([False] * bsz, device=device)\n",
    "input_text_mask = tokens != tokenizer.pad_id\n",
    "\n",
    "# Pre-allocate KV Cache.\n",
    "# Notice how we reserve `max_seq_len` length of tokens per request.\n",
    "# Other requests cannot use this space, leading to internal fragmentation.\n",
    "kv_cache = [(torch.randn((bsz, max_seq_len, n_kv_heads, head_dim), dtype=torch.bfloat16, device=device),\n",
    "             torch.randn((bsz, max_seq_len, n_kv_heads, head_dim), dtype=torch.bfloat16, device=device)) for _ in range(n_layers)]\n",
    "\n",
    "# Do inference\n",
    "for cur_pos in range(min_prompt_len, max_seq_len):\n",
    "    next_token = forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "    next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "    tokens[:, cur_pos] = next_token\n",
    "\n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (torch.isin(next_token, stop_tokens))\n",
    "    prev_pos = cur_pos\n",
    "\n",
    "    if all(eos_reached):\n",
    "        break\n",
    "\n",
    "# Print generated answers / calculate fragmented memory size\n",
    "fragmented_memory_size = 0\n",
    "for i, toks in enumerate(tokens.tolist()):\n",
    "    start = 0 if False else len(prompt_tokens[i])\n",
    "    toks = toks[start: len(prompt_tokens[i]) + max_seq_len]\n",
    "    for stop_token in tokenizer.stop_tokens:\n",
    "        try:\n",
    "            eos_idx = toks.index(stop_token)\n",
    "            toks = toks[:eos_idx]\n",
    "            fragmented_memory_size += (max_seq_len - eos_idx) * n_kv_heads * head_dim * 2 * 2 * n_layers\n",
    "        except ValueError:\n",
    "            pass\n",
    "    print(tokenizer.decode(toks))\n",
    "    print('-'*50)\n",
    "\n",
    "# Print fragmented memory size and percentage\n",
    "fragmented_ratio = fragmented_memory_size / torch.cuda.get_device_properties(0).total_memory\n",
    "print(f'Fragmented Memory: {fragmented_memory_size / 1e9:.2f} GB ({fragmented_ratio * 100:.2f}%)')\n"
   ],
   "id": "e049a62ec440b956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c167652c20a3903"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
